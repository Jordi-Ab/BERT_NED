{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed1c476c-79de-4457-81bb-9b4c15977eb9",
   "metadata": {},
   "source": [
    "# Build Data set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b044a4b4-f56c-4a32-bbd9-8d543ee8b290",
   "metadata": {},
   "source": [
    "## About the DataSet\n",
    "\n",
    "DataSet consists of news articles scrapped from a Mexican Newspaper. For each news article I extracted Entities using Stanford Core NLP Python client. I focused only on PERSON, ORGANIZATION, COUNTRY and LOCATION entities.\n",
    "\n",
    "For each entity of interest I performed a query string search to Wikidata knowledge graph, which sometimes returned several results for each entity. The objective is to train a \"lightweight\" (lighter than an LLM)  model that is able to identify which of all the wikidata options is the correct option given the context in which the entity is being mentioned.\n",
    "\n",
    "I built a dataset of x articles, x entities and y options. The true labels were computed using StabilityAI/StableBeluga-7B LLM. I left the LLM running for several days answering the query:\n",
    "```\n",
    "\"\"\"\n",
    "Given this news article:\n",
    "\n",
    "\"{news_text}\"\n",
    "\n",
    "When the article mentions \"{entity}\", which of the following options is the news article most likely referring to? Provide only one option.\n",
    "options:\n",
    "\n",
    "{search_options}\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "in order to get a big enough dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6a60e1b-b1b9-4350-87d4-b0fa323b7b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer\n",
    "import json\n",
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74a5183b-88bd-4668-a413-b08b8d1e1037",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_progress_bar(iteration, total, bar_length=50):\n",
    "    progress = float(iteration) / float(total)\n",
    "    arrow = '=' * int(round(progress * bar_length) - 1)\n",
    "    spaces = ' ' * (bar_length - len(arrow))\n",
    "\n",
    "    print(f'Progress: [{arrow + spaces}] {int(progress * 100)}%', end='\\r')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fda39fe-af77-483e-907c-7251c1664b62",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d528d874-af9b-4052-a5b6-ee05223d0330",
   "metadata": {},
   "source": [
    "`sb_disambiguation_result.json` is an 800 MB file which will not be uploaded to github. Contains the teacher observations produced using `StableBeluga - 7b`. See the `0-ask_stable_beluga.ipynb` notebook for further details on how this dataset was produced using LLMs.\n",
    "The StableBeluga process was ran on a subset of 315,512 news-entities observations. We considered this as a big enough data set for a POC. The process could further be left running for the whole 2,953,563 samples or for a bigger subset. It is important to left out a porcetage of the whole observations for validations and to compare running times.\n",
    "\n",
    "This 315,512 subset of news-entities observations will be our training data.\n",
    "\n",
    "Columns:\n",
    "\n",
    "- `text`: Is the text of the Named Entity. For example, Donald Trump.\n",
    "- `ner`: Is the type of the Named Entity, such as PERSON, LOCATION, etc.\n",
    "- `nerConfidences`: Confidence of the Named Entity belonging to a NER.\n",
    "- `clean_text`: Is the text of the Named Entity cleaned regarding special caracters and upper cased.\n",
    "- `h1`: Is the title of the news articles. Is used as key to merge with the news articles information.\n",
    "- `wikidata_search_entries`: Is a list of all instances of an entity that were found in Wikidata. Only one of those instances corresponds to the correct option.\n",
    "- `sb_answer`: Is the answer of Stable Belgua answering the question:\n",
    "```\n",
    "  \"\"\"\n",
    "Given this news article:\n",
    "\n",
    "\"{news_text}\"\n",
    "\n",
    "When the article mentions \"{entity}\", which of the following options is the news article most likely referring to? Provide only one option.\n",
    "options:\n",
    "\n",
    "{search_options}\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "`news_with_metadata.json` is a 1.5 GB file which will not be uploaded to github. Contains 117,786 spanish news articles and metadata associated to those news articles, which was also extracted using other NLP techniques, such as translations and summarizations. Data was scrapped from \"El Universal\" digital news paper.\n",
    "\n",
    "Relevant Columns:\n",
    "\n",
    "- `h1`: Is the title of the news articles. Is used as key to merge with the news articles information.\n",
    "- `date`: Date and time when the news articles was published.\n",
    "- `author`: Name of the author who published the article.\n",
    "- `content`: Article text in spanish.\n",
    "- `h1_en`: Article title translated to english.\n",
    "- `content_en`: Article text translated to English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13678740-4515-4171-a3f9-96acaa818410",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"datasets/sb_disambiguation_result.json\", \"r\") as file:\n",
    "    sb_results = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3be6dd2-5b80-462c-bcc9-0e8fc5bef458",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "315512"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \n",
    "len(sb_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8cd66e68-0f91-4f0d-9092-083473e9b907",
   "metadata": {},
   "outputs": [],
   "source": [
    "sb_results_df = pd.DataFrame.from_dict(sb_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba84e906-09e1-4c7f-a0cd-857269feff68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>ner</th>\n",
       "      <th>nerConfidences</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>wikidata_search_entries</th>\n",
       "      <th>h1</th>\n",
       "      <th>sb_answer</th>\n",
       "      <th>options_given</th>\n",
       "      <th>ix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tokyo</td>\n",
       "      <td>CITY</td>\n",
       "      <td>{'LOCATION': 0.9994902892958}</td>\n",
       "      <td>TOKYO</td>\n",
       "      <td>[{'id': 'Q1490', 'display_label': 'Tokyo', 'di...</td>\n",
       "      <td>El Hijo de Dr. Wagner Jr. gana la ‘Global Tag ...</td>\n",
       "      <td>The news article is most likely referring to: ...</td>\n",
       "      <td>1. Tokyo, capital and largest city of Japan \\n...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Duprée</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>{'PERSON': 0.83429111680194}</td>\n",
       "      <td>DUPREE</td>\n",
       "      <td>[{'id': 'Q1013540', 'display_label': 'Dupree',...</td>\n",
       "      <td>El Hijo de Dr. Wagner Jr. gana la ‘Global Tag ...</td>\n",
       "      <td>The news article is most likely referring to: ...</td>\n",
       "      <td>1. Dupree, city in South Dakota, United States...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Katsuhiko Nakajima</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>{'PERSON': 0.99989643856346}</td>\n",
       "      <td>KATSUHIKO NAKAJIMA</td>\n",
       "      <td>[{'id': 'Q959636', 'display_label': 'Katsuhiko...</td>\n",
       "      <td>El Hijo de Dr. Wagner Jr. gana la ‘Global Tag ...</td>\n",
       "      <td>The news article is most likely referring to: ...</td>\n",
       "      <td>1. Katsuhiko Nakajima, Japanese professional w...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ministry of Finance and Public Credit</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "      <td>{'ORGANIZATION': 0.78478234268622}</td>\n",
       "      <td>MINISTRY OF FINANCE AND PUBLIC CREDIT</td>\n",
       "      <td>[{'id': 'Q3062474', 'display_label': 'Ministry...</td>\n",
       "      <td>Estímulos a gasolinas afectan recaudación entr...</td>\n",
       "      <td>The news article is most likely referring to: ...</td>\n",
       "      <td>1. Ministry of Finance and Public Credit, gove...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Treasury</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "      <td>{'ORGANIZATION': 0.99823209224661}</td>\n",
       "      <td>TREASURY</td>\n",
       "      <td>[{'id': 'Q3277092', 'display_label': 'Departme...</td>\n",
       "      <td>Estímulos a gasolinas afectan recaudación entr...</td>\n",
       "      <td>The news article is most likely referring to: ...</td>\n",
       "      <td>1. Department of the Treasury, Australian gove...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    text           ner  \\\n",
       "0                                  Tokyo          CITY   \n",
       "1                                 Duprée        PERSON   \n",
       "2                     Katsuhiko Nakajima        PERSON   \n",
       "3  Ministry of Finance and Public Credit  ORGANIZATION   \n",
       "4                               Treasury  ORGANIZATION   \n",
       "\n",
       "                       nerConfidences                             clean_text  \\\n",
       "0       {'LOCATION': 0.9994902892958}                                  TOKYO   \n",
       "1        {'PERSON': 0.83429111680194}                                 DUPREE   \n",
       "2        {'PERSON': 0.99989643856346}                     KATSUHIKO NAKAJIMA   \n",
       "3  {'ORGANIZATION': 0.78478234268622}  MINISTRY OF FINANCE AND PUBLIC CREDIT   \n",
       "4  {'ORGANIZATION': 0.99823209224661}                               TREASURY   \n",
       "\n",
       "                             wikidata_search_entries  \\\n",
       "0  [{'id': 'Q1490', 'display_label': 'Tokyo', 'di...   \n",
       "1  [{'id': 'Q1013540', 'display_label': 'Dupree',...   \n",
       "2  [{'id': 'Q959636', 'display_label': 'Katsuhiko...   \n",
       "3  [{'id': 'Q3062474', 'display_label': 'Ministry...   \n",
       "4  [{'id': 'Q3277092', 'display_label': 'Departme...   \n",
       "\n",
       "                                                  h1  \\\n",
       "0  El Hijo de Dr. Wagner Jr. gana la ‘Global Tag ...   \n",
       "1  El Hijo de Dr. Wagner Jr. gana la ‘Global Tag ...   \n",
       "2  El Hijo de Dr. Wagner Jr. gana la ‘Global Tag ...   \n",
       "3  Estímulos a gasolinas afectan recaudación entr...   \n",
       "4  Estímulos a gasolinas afectan recaudación entr...   \n",
       "\n",
       "                                           sb_answer  \\\n",
       "0  The news article is most likely referring to: ...   \n",
       "1  The news article is most likely referring to: ...   \n",
       "2  The news article is most likely referring to: ...   \n",
       "3  The news article is most likely referring to: ...   \n",
       "4  The news article is most likely referring to: ...   \n",
       "\n",
       "                                       options_given   ix  \n",
       "0  1. Tokyo, capital and largest city of Japan \\n...  NaN  \n",
       "1  1. Dupree, city in South Dakota, United States...  NaN  \n",
       "2  1. Katsuhiko Nakajima, Japanese professional w...  NaN  \n",
       "3  1. Ministry of Finance and Public Credit, gove...  NaN  \n",
       "4  1. Department of the Treasury, Australian gove...  NaN  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sb_results_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4f812e9-a3f5-4058-88fd-9a4d68dd1e3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Tokyo',\n",
       " 'ner': 'CITY',\n",
       " 'nerConfidences': \"{'LOCATION': 0.9994902892958}\",\n",
       " 'clean_text': 'TOKYO',\n",
       " 'wikidata_search_entries': [{'id': 'Q1490',\n",
       "   'display_label': 'Tokyo',\n",
       "   'display_desc': 'capital and largest city of Japan',\n",
       "   'label': 'Tokyo',\n",
       "   'desc': 'capital and largest city of Japan',\n",
       "   'label_desc': 'Tokyo, capital and largest city of Japan',\n",
       "   'match_type': 'label'},\n",
       "  {'id': 'Q7473516',\n",
       "   'display_label': 'Tokyo',\n",
       "   'display_desc': 'special wards in the eastern part of Tokyo Metropolis in Japan, that used to form a single city',\n",
       "   'label': 'Tokyo',\n",
       "   'desc': 'special wards in the eastern part of Tokyo Metropolis in Japan, that used to form a single city',\n",
       "   'label_desc': 'Tokyo, special wards in the eastern part of Tokyo Metropolis in Japan, that used to form a single city',\n",
       "   'match_type': 'label'},\n",
       "  {'id': 'Q7842',\n",
       "   'display_label': 'University of Tokyo',\n",
       "   'display_desc': 'National university in Tokyo, Japan',\n",
       "   'label': 'University of Tokyo',\n",
       "   'desc': 'National university in Tokyo, Japan',\n",
       "   'label_desc': 'University of Tokyo, National university in Tokyo, Japan',\n",
       "   'match_type': 'alias'},\n",
       "  {'id': 'Q8420',\n",
       "   'display_label': '1964 Summer Olympics',\n",
       "   'display_desc': 'Games of the XVIII Olympiad, in Tokyo, Japan',\n",
       "   'label': '1964 Summer Olympics',\n",
       "   'desc': 'Games of the XVIII Olympiad, in Tokyo, Japan',\n",
       "   'label_desc': '1964 Summer Olympics, Games of the XVIII Olympiad, in Tokyo, Japan',\n",
       "   'match_type': 'alias'},\n",
       "  {'id': 'Q1065186',\n",
       "   'display_label': 'Tokyo',\n",
       "   'display_desc': 'song by Yui',\n",
       "   'label': 'Tokyo',\n",
       "   'desc': 'song by Yui',\n",
       "   'label_desc': 'Tokyo, song by Yui',\n",
       "   'match_type': 'label'},\n",
       "  {'id': 'Q41187',\n",
       "   'display_label': 'Sony',\n",
       "   'display_desc': 'Japanese multinational conglomerate corporation',\n",
       "   'label': 'Sony',\n",
       "   'desc': 'Japanese multinational conglomerate corporation',\n",
       "   'label_desc': 'Sony, Japanese multinational conglomerate corporation',\n",
       "   'match_type': 'alias'},\n",
       "  {'id': 'Q181278',\n",
       "   'display_label': '2020 Summer Olympics',\n",
       "   'display_desc': 'Games of the XXXII Olympiad, in Tokyo, Japan, held in 2021',\n",
       "   'label': '2020 Summer Olympics',\n",
       "   'desc': 'Games of the XXXII Olympiad, in Tokyo, Japan, held in 2021',\n",
       "   'label_desc': '2020 Summer Olympics, Games of the XXXII Olympiad, in Tokyo, Japan, held in 2021',\n",
       "   'match_type': 'alias'}],\n",
       " 'h1': 'El Hijo de Dr. Wagner Jr. gana la ‘Global Tag Team’ en Japón\\xa0',\n",
       " 'sb_answer': 'The news article is most likely referring to: \\n\\n1. Tokyo, capital and largest city of Japan',\n",
       " 'options_given': '1. Tokyo, capital and largest city of Japan \\n2. Tokyo, special wards in the eastern part of Tokyo Metropolis in Japan, that used to form a single city \\n3. University of Tokyo, National university in Tokyo, Japan \\n4. 1964 Summer Olympics, Games of the XVIII Olympiad, in Tokyo, Japan \\n5. Tokyo, song by Yui \\n6. Sony, Japanese multinational conglomerate corporation \\n7. 2020 Summer Olympics, Games of the XXXII Olympiad, in Tokyo, Japan, held in 2021 \\n',\n",
       " 'ix': nan}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sb_results_df.iloc[0].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "75af0f2c-50cc-4124-9723-31a705195968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the news articles data set\n",
    "path_file = 'datasets/news_with_metadata.json'\n",
    "with open(path_file, 'r') as jfile:\n",
    "    processed_news_articles = json.load(jfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0dcd65a3-dd61-43d5-9b66-d1eaf879b436",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_articles_df = pd.DataFrame.from_dict(processed_news_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "95d1ad1f-69b3-4bd7-bd65-281f7f1ff36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ixs = sb_results_df.reset_index()['index'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6969ce9e-d03c-4fde-8c6f-682c700b42ba",
   "metadata": {},
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2e3679b4-8764-4b60-b07c-64bef77ff4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the article text into the stable beluga teacher observations\n",
    "sb_results_df = pd.merge(\n",
    "    sb_results_df,\n",
    "    news_articles_df[['h1', 'content_en']],\n",
    "    how='left',\n",
    "    on='h1'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9f5b47-2ffe-462f-8e91-5256fa9dce32",
   "metadata": {},
   "source": [
    "## Extract index of the best option given by Stable Beluga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "87d7ac98-7752-4d0e-907e-bc4bad4169e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_pattern = \"The news article is most likely referring to:\"\n",
    "sb_results_df['sb_option_given'] = sb_results_df['sb_answer'].str[len(regex_pattern):]\n",
    "sb_results_df['sb_option_given'] = sb_results_df['sb_option_given'].str.replace('\\n', '').str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "499013bc-6a55-4f0d-b8ec-26ff3305b1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_pattern = r\"(\\d+.)(.*)\"\n",
    "sb_results_df[['index_of_option_given', 'sb_option_given']] = sb_results_df['sb_option_given'].str.extract(regex_pattern)\n",
    "sb_results_df['index_of_option_given'] = sb_results_df['index_of_option_given'].str.replace(\".\", \"\").str.strip()\n",
    "sb_results_df['sb_option_given'] = sb_results_df['sb_option_given'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "67b5b703-1a2f-4b15-868f-9433d6f879dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>ner</th>\n",
       "      <th>nerConfidences</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>wikidata_search_entries</th>\n",
       "      <th>h1</th>\n",
       "      <th>sb_answer</th>\n",
       "      <th>options_given</th>\n",
       "      <th>ix</th>\n",
       "      <th>content_en</th>\n",
       "      <th>sb_option_given</th>\n",
       "      <th>index_of_option_given</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [text, ner, nerConfidences, clean_text, wikidata_search_entries, h1, sb_answer, options_given, ix, content_en, sb_option_given, index_of_option_given]\n",
       "Index: []"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see if there were instances where the process couldn't find a numeric best option\n",
    "sb_results_df[sb_results_df['index_of_option_given'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ed871c70-f3e1-4572-bab0-54b7a7f6a001",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_regex_pattern(term):\n",
    "    \"\"\"\n",
    "    Define a function that creates a regex pattern with word boundaries in order to avoid\n",
    "    false positives when searching for an entity inside a text. Examples of false positives are:\n",
    "    entity: 'us' (as united states)\n",
    "    false positive: `museum`, which contains `us` in the text.\n",
    "    \"\"\"\n",
    "    return r'\\b' + re.escape(term) + r'\\b'\n",
    "\n",
    "def get_sentences_containing_entity(row):\n",
    "    \"\"\"\n",
    "    Function to be used over a DataFrame.\n",
    "    ---------\n",
    "    For an article text which is in the DataFrame column named `content_en`\n",
    "    extract all sentences that mention an entity contained in the DataFrame column named `text`\n",
    "    \"\"\"\n",
    "    # article text\n",
    "    content = row['content_en']\n",
    "    # the entity\n",
    "    ent = row['text']\n",
    "    # break text into sentences\n",
    "    sents_srs = pd.Series(nltk.sent_tokenize(content))\n",
    "    # create regex patter with word boundaries\n",
    "    reg = create_regex_pattern(ent)\n",
    "    # find sentences containing entity\n",
    "    sents_containing_ent = sents_srs[\n",
    "        sents_srs.str.contains(reg, regex=True)\n",
    "    ].tolist()\n",
    "    return sents_containing_ent\n",
    "    \n",
    "# this is a slow process and could be optimized\n",
    "sb_results_df['sentence_mentions']=sb_results_df.apply(\n",
    "    lambda row: get_sentences_containing_entity(row), \n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a5d77da5-c39d-4006-94d2-28be466758e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sb_results_df['label_desc_options'] = sb_results_df['wikidata_search_entries'].apply(\n",
    "    lambda x: [i.get('label_desc') for i in x]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bb304c8b-8549-43b6-818d-61083fc87ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sb_results_df['sents_len'] = sb_results_df['sentence_mentions'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d07b97fd-4e52-4e93-b2b5-bbe90919941e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop observations with no sentence mentions\n",
    "sb_results_df.drop(sb_results_df[sb_results_df['sents_len']==0].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "68fa65d2-b4a8-467c-a943-1a2f042b47b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sb_results_df = sb_results_df.reset_index(drop=True).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "370d5994-0352-41be-9016-dc3c3e5149e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# explode by sentence mentions, so that each entity has only one sentence mention\n",
    "sb_results_df_exp = sb_results_df.set_index(\n",
    "    ['index']\n",
    ").explode(\n",
    "    'sentence_mentions'\n",
    ").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f3304416-ae6a-43be-bea3-82b3b76c8653",
   "metadata": {},
   "outputs": [],
   "source": [
    "sb_results_df_exp['index_of_option_given'] = (sb_results_df_exp['index_of_option_given'].astype(int) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5947a56b-d278-4e3c-9c02-d58e62fdf41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = sb_results_df_exp[\n",
    "    ['index', 'text', 'index_of_option_given', 'sentence_mentions', 'label_desc_options']\n",
    "].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c8f9c6-bf13-4238-b898-55c19d44c120",
   "metadata": {},
   "source": [
    "# Encode Data\n",
    "\n",
    "In this part, we create the input texts, as well as the labels that will be used to train the BERT model. We make sure that the input text does not exceed 512 tokens (max number of tokens that BERT accepts).\n",
    "\n",
    "Example of an input text:\n",
    "\n",
    "Is \"Andres Manuel Lopez Obrador\" in the context of: \"President Andres Manuel Lopez Obrador's call for the next scheduled on 27 November ...\", referring to [SEP] \"Andrés Manuel López Obrador, President of Mexico since 2018\"?\n",
    "\n",
    "The sentence where the entity is being mentioned might be too long, resulting in input texts with more than 512 tokens. In this case, we shorten the sentence by extracting only the surrounding text in the vicinity of the entity mention, making sure that the surroiunding text doesnt exceed 512 tokens again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5d624602-6cef-48c3-8a4b-2b26e35dad72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ba44af5cd394cc3aa18a9a77c2040d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\jordi\\venvs\\bart\\Lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in D:\\Users\\jordi\\.cache\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d11b19c0d244fd8924166cccde6d4af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78477806cf5a4ea58f26ba7acdf18551",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5147eb702d5d46f197d5e690cfcf9820",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the tokenizer.\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bed79446-2fe9-40f6-8f6b-3d4a5d28c729",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def build_query_for_bert_ned(\n",
    "    entity_text, sentence_mention, option\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a query string that will be feed to BERT.\n",
    "    -------\n",
    "    - entity_text: str: Named entity.\n",
    "    - sentence_mention: str: Sentence where the entity is being mentioned (context).\n",
    "    - option: str: The wikidata option.\n",
    "    \"\"\"\n",
    "    query = \" Is '{entity_mention}' in the context of: '{sentence_mention}', referring to [SEP] {option}?\".format(\n",
    "        entity_mention=entity_text,\n",
    "        sentence_mention=sentence_mention,\n",
    "        option=option\n",
    "    )\n",
    "    return query\n",
    "\n",
    "def join_text_tokens(tokens):\n",
    "    \"\"\"\n",
    "    Join a tokenized text. Puntuaction marks are not followed by a space, while all other tokens are\n",
    "    followed by a space\n",
    "    \"\"\"\n",
    "    text = ''.join(\n",
    "        [\n",
    "            tokens[i] if tokens[i] in string.punctuation else ' ' + tokens[i] for i in range(len(tokens))\n",
    "        ]\n",
    "    ).strip()\n",
    "    return text\n",
    "\n",
    "def find_surrounding_text(target_word, sentence, n_tokens=5):\n",
    "    \"\"\"\n",
    "    Find text in the vicinity of an entity mention.\n",
    "    -----------------------\n",
    "    - target_word: str: Named entity\n",
    "    - sentence: str: The whole sentence where the entity is being mentioned\n",
    "    - n_tokens: int: Number of tokens surrounding the `target_word`. n_tokens + target_word + n_tokens\n",
    "    \"\"\"\n",
    "    start_indices = [\n",
    "        m.start() for m in re.finditer(\n",
    "            r'\\b' + re.escape(target_word) + r'\\b', \n",
    "            sentence, \n",
    "            flags=re.IGNORECASE)\n",
    "    ]\n",
    "    \n",
    "    surrounding_words = []\n",
    "    for ix in start_indices:\n",
    "        before_txt = sentence[:ix]\n",
    "        before_tkns = nltk.word_tokenize(before_txt)\n",
    "        before_n_tokens = before_tkns[-n_tokens:]\n",
    "        after_txt = sentence[ix+len(target_word):]\n",
    "        after_tkns = nltk.word_tokenize(after_txt)\n",
    "        after_n_tokens = after_tkns[:n_tokens]\n",
    "        all_tokens = before_n_tokens + [target_word] + after_n_tokens \n",
    "        surrounding_txt = join_text_tokens(all_tokens)\n",
    "        surrounding_words.append(surrounding_txt)\n",
    "    return surrounding_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bc243c4f-ddb7-4334-9a28-f622363ec462",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataset = final_df.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d8bfcb8e-db02-40a5-9760-27c9ea339d8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: [==                                                ] 5%\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (553 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: [================================================= ] 99%\r"
     ]
    }
   ],
   "source": [
    "# Create data set of inputs less than 512 tokens\n",
    "\n",
    "new_final_ds = []\n",
    "total_exs = len(final_dataset)\n",
    "for e, example in enumerate(final_dataset):\n",
    "    print_progress_bar(iteration=e, total=total_exs)\n",
    "    # For each option, we'll need to format the text in a way that BERT can understand.\n",
    "    # This usually involves concatenating the mention sentence(s) with the option text,\n",
    "    # separated by the [SEP] token, and starting with a [CLS] token.\n",
    "    for i, option in enumerate(example['label_desc_options']):\n",
    "        new_entry = dict()\n",
    "        entity_mention = example['text']\n",
    "        new_entry['entity_mention'] = entity_mention\n",
    "        sentence_mention = example['sentence_mentions']\n",
    "        option_ix = example['index_of_option_given']\n",
    "\n",
    "        # n_tokens surrounding text in case that input text exceed 512 tokens\n",
    "        n_tokens = 30\n",
    "        while True: # do this until encoded input text doesn't exceed 512 tokens\n",
    "            # input text\n",
    "            query = build_query_for_bert_ned(\n",
    "                entity_mention, sentence_mention, option\n",
    "            )\n",
    "            # try encoding the query\n",
    "            encoded_dict = tokenizer.encode_plus(\n",
    "                query,                           # Sentence to encode.\n",
    "                add_special_tokens = True,       # Add '[CLS]' and '[SEP]'\n",
    "                #max_length = 512,                # Pad & truncate all sentences.\n",
    "                #padding='max_length',            # Make sure this applies padding as needed\n",
    "                #truncation=True,\n",
    "                #return_attention_mask = True,    # Construct attention masks.\n",
    "                #return_tensors = 'pt',           # Return pytorch tensors.\n",
    "            )\n",
    "            if len(encoded_dict['input_ids'])<=512:\n",
    "                # encoded query didn't exceeded 512 tokens\n",
    "                new_entry['bert_qry'] = query\n",
    "                new_entry['sentence_mention'] = sentence_mention\n",
    "                new_entry['option'] = option\n",
    "                new_entry['label'] = int(i == option_ix)\n",
    "                break # break the while true\n",
    "            \n",
    "            # encoded query exceeded 512 tokens, keep the while loop\n",
    "\n",
    "            # find the text around the entity in order to shorten the input text\n",
    "            sentence_mentions = find_surrounding_text(\n",
    "                target_word=entity_mention, \n",
    "                sentence=sentence_mention, \n",
    "                n_tokens=n_tokens\n",
    "            )\n",
    "            sentence_mention = 'None' if len(sentence_mentions)==0 else sentence_mentions[0]\n",
    "            # decrease the number of tokens around the text by 5 tokens.\n",
    "            n_tokens-= 5\n",
    "        \n",
    "        new_final_ds.append(new_entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efda0da-7361-4a67-a831-c0583ef08312",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_final_df = pd.DataFrame.from_dict(new_final_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bbba25-ff9f-4cd0-bb76-13378e108c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the data set containing input texts and labels.\n",
    "new_final_df.to_json(\"datasets/dataset_for_bert_fine_tune_shortened.json\", orient=\"records\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
