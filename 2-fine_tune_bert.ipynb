{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65289d76-9630-486a-9c9f-b8473c8db98d",
   "metadata": {},
   "source": [
    "# Fine Tune BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1b2987-164a-4f41-a924-e1eac0e706ca",
   "metadata": {},
   "source": [
    "In this Notebook we perform a fine tunning of BERT for binary classification.\n",
    "\n",
    "Code was extracted from this tutorial:\n",
    "\n",
    "https://colab.research.google.com/github/Ankur3107/colab_notebooks/blob/master/classification/BERT_Fine_Tuning_Sentence_Classification_v2.ipynb#scrollTo=6J-FYdx6nFE_\n",
    "\n",
    "The task that we will be trying to train BERT to solve is Named Entity Dissambiguation. Namely, given an entity and different options of possible entities, and given the context of which the entity is being mentioned, which of the options in the context referring to?\n",
    "\n",
    "For example, the entity \"Jaguar\" has a lot of options:\n",
    "\n",
    "- Jaguar as an animal\n",
    "- Jaguar as a brand of cars\n",
    "- Jaguar as a supercomputer\n",
    "- etc..\n",
    "\n",
    "So, given the context: \n",
    " - \"The man saw a Jaguar speed in the highway\" -> the context is most likely referring to \"Jaguar\" as a car.\n",
    " - \"The prey saw the jaguar cross the jungle\" -> The context is most likely referring to \"jaguar\" as an animal.\n",
    "\n",
    "We formulated this as a binary classification problem, giving BERT all entities on a dataset, as long as the context on which the entity is being mentioned, the option and the label 1/0 if it is correct of incorrect.\n",
    "\n",
    "The dataset consists of all combinations of the different options for an entity and mentions in a sentence, along with a 0/1 label if the entity option is correct for a certain sentence mention (context). For more information on how this dataset was built to fine tune the model, see `1-make_dataset.ipynb` notebook. Each \"bert_qry\" input text was shortened in order to avoid input texts that exceed 512 tokens.\n",
    "\n",
    "For example, in this Jaguar example, the Data Set would look like:\n",
    "```\n",
    "]\n",
    "    }\n",
    "    \"bert_qry\": \"Is 'Jaguar' in the context of: 'The man saw a Jaguar speed in the highway', referring to [SEP] Jaguar as an animal?\",\n",
    "    \"label\": 0,\n",
    "    },\n",
    "}\n",
    "    \"bert_qry\": \"Is 'Jaguar' in the context of: 'The man saw a Jaguar speed in the highway', referring to [SEP] Jaguar as a brand of cars?\",\n",
    "    \"label\": 1,\n",
    "    },\n",
    "}\n",
    "    \"bert_qry\": \"Is 'Jaguar' in the context of: 'The man saw a Jaguar speed in the highway', referring to [SEP] Jaguar as a supercomputer?\",\n",
    "    \"label\": 0,\n",
    "    },\n",
    "}\n",
    "    \"bert_qry\": \"Is 'Jaguar' in the context of: 'The prey saw the jaguar cross the jungle', referring to [SEP] Jaguar as an animal?\",\n",
    "    \"label\": 1,\n",
    "    },\n",
    "}\n",
    "    \"bert_qry\": \"Is 'Jaguar' in the context of: 'The prey saw the jaguar cross the jungle', referring to [SEP] Jaguar as a brand of cars?\",\n",
    "    \"label\": 0,\n",
    "    },\n",
    "}\n",
    "    \"bert_qry\": \"Is 'Jaguar' in the context of: 'The prey saw the jaguar cross the jungle', referring to [SEP] Jaguar as a supercomputer?\",\n",
    "    \"label\": 0,\n",
    "    }\n",
    "]\n",
    "```\n",
    "\n",
    "About the DataSet used for this training:\n",
    "\n",
    "Our Dataset is comprised of news articles scrapped from a Mexican Newspaper. NER was applied to each article, so we got all Entities mentioned in the article. For each Entity, we performed a query to WikiData in order to extract all possible options for an Entity. An LLM was used as a teacher in order to disambiguate a subset of the whole dataset (`0-ask_stable_beluga.ipynb`) The teacher observations were used to get the labels. A preprocessing was performed in order to create the \"bert_qry\", making sure it doesn't exceed 512 tokens (`1-make_dataset.ipynb`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54bbafa4-7470-43b1-b655-7a70e5fb3a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset, random_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b7d65fe-4bda-4aab-aa7a-a5c960d9cc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_progress_bar(iteration, total, bar_length=50):\n",
    "    progress = float(iteration) / float(total)\n",
    "    arrow = '=' * int(round(progress * bar_length) - 1)\n",
    "    spaces = ' ' * (bar_length - len(arrow))\n",
    "\n",
    "    print(f'Progress: [{arrow + spaces}] {int(progress * 100)}%', end='\\r')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4feec9-3876-49da-b289-3b6409854b4c",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e1fa14e-6744-4f64-bc80-4d0176f40501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This data set has been previously hand crafted. Performing the entity mentions\n",
    "# and the sentence where the entity is mentioned as context in order to provide that\n",
    "# to BERT as input. The bert queries were also shortened in order to ensure\n",
    "# than they don't exceed 512 tokens length.\n",
    "with open(\"datasets/dataset_for_bert_fine_tune_shortened.json\", \"r\") as file:\n",
    "    data_for_bert_fine_tunning = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5393b7b-a794-425b-b235-9face942d7f9",
   "metadata": {},
   "source": [
    "## Tokenize and Encode the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9f87cd9-72dc-40a1-b8a7-699f6b85ab2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer.\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eed95160-c608-4822-a485-cde5c7368937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: [================================================= ] 99%\r"
     ]
    }
   ],
   "source": [
    "total_exs = len(data_for_bert_fine_tunning)\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "labels = []  # This should be 0 for incorrect options and 1 for the correct one.\n",
    "for e, example in enumerate(data_for_bert_fine_tunning):\n",
    "    \n",
    "    print_progress_bar(iteration=e, total=total_exs)\n",
    "    query = example['bert_qry']\n",
    "    \n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "        query,                           # Sentence to encode.\n",
    "        add_special_tokens = True,       # Add '[CLS]' and '[SEP]'\n",
    "        max_length = 512,                # Set the max length to 512\n",
    "        padding='max_length',            # Pad to max length (512)\n",
    "        truncation=True,                 # Truncate when greater than 512\n",
    "        return_attention_mask = True,    # Construct attention masks.\n",
    "        return_tensors = 'pt',           # Return pytorch tensors.\n",
    "    )\n",
    "\n",
    "    # Add the encoded sentence to the list.    \n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    # And its attention mask (simply differentiates padding from non-padding).\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "    labels.append(example[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ea86e38-44a4-4087-8aa9-8c698322f0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "be7953f7-d105-43b1-92fc-1381b78c974c",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Convert the lists into tensors.\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89d92604-bd08-4d13-b062-7af7eec58e41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3360112, 512]), torch.Size([3360112, 512]), torch.Size([3360112]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids.shape, attention_masks.shape, labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f584212-1ff5-4084-983f-969b318aac8b",
   "metadata": {},
   "source": [
    "## Create and Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "49291b11-e00e-4669-911e-85c9a19139b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the dataset into a DataLoader - combines a dataset and a sampler, and provides an iterable over the dataset.\n",
    "batch_size = 16  # You might need to adjust this depending on your GPU.\n",
    "\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(\n",
    "    dataset, \n",
    "    [train_size, val_size]\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,  # The training samples.\n",
    "    sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "    batch_size = batch_size\n",
    ")\n",
    "\n",
    "validation_dataloader = DataLoader(\n",
    "    val_dataset, # The validation samples.\n",
    "    sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
    "    batch_size = batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b87f30-7e85-41c3-89b3-2d32ac38999b",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5a0f2d79-f599-438e-b4c6-0ea78dd86b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tell pytorch to run this model on the GPU.\n",
    "device_to_use = \"cuda:0\"\n",
    "device = torch.device(device_to_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d87ba363-3690-4972-a4a2-38b6b5ed9ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-cased\", \n",
    "    num_labels = 2,  # Binary classification, 1/0.\n",
    "    output_attentions = False, \n",
    "    output_hidden_states = False, \n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963e2463-dbec-4d89-8b7d-233096c8e505",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a68e43f9-4cda-4426-a01c-394db5555c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr = 2e-5, # args.learning_rate\n",
    "    eps = 1e-8 # args.adam_epsilon\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "74f1f0e5-b15d-4d99-9bcb-2566bd8fbec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Number of training epochs (authors recommend between 2 and 4)\n",
    "epochs = 4\n",
    "\n",
    "# Total number of training steps is number of batches * number of epochs.\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, \n",
    "    num_warmup_steps = 0, # Default value in run_glue.py\n",
    "    num_training_steps = total_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c83aabbc-ab39-4b8c-92a7-b10db38afc91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 4 ========\n",
      "  Batch 2,000  of  168,006.\n",
      "  Batch 4,000  of  168,006.\n",
      "  Batch 6,000  of  168,006.\n",
      "  Batch 8,000  of  168,006.\n",
      "  Batch 10,000  of  168,006.\n",
      "  Batch 12,000  of  168,006.\n",
      "  Batch 14,000  of  168,006.\n",
      "  Batch 16,000  of  168,006.\n",
      "  Batch 18,000  of  168,006.\n",
      "  Batch 20,000  of  168,006.\n",
      "  Batch 22,000  of  168,006.\n",
      "  Batch 24,000  of  168,006.\n",
      "  Batch 26,000  of  168,006.\n",
      "  Batch 28,000  of  168,006.\n",
      "  Batch 30,000  of  168,006.\n",
      "  Batch 32,000  of  168,006.\n",
      "  Batch 34,000  of  168,006.\n",
      "  Batch 36,000  of  168,006.\n",
      "  Batch 38,000  of  168,006.\n",
      "  Batch 40,000  of  168,006.\n",
      "  Batch 42,000  of  168,006.\n",
      "  Batch 44,000  of  168,006.\n",
      "  Batch 46,000  of  168,006.\n",
      "  Batch 48,000  of  168,006.\n",
      "  Batch 50,000  of  168,006.\n",
      "  Batch 52,000  of  168,006.\n",
      "  Batch 54,000  of  168,006.\n",
      "  Batch 56,000  of  168,006.\n",
      "  Batch 58,000  of  168,006.\n",
      "  Batch 60,000  of  168,006.\n",
      "  Batch 62,000  of  168,006.\n",
      "  Batch 64,000  of  168,006.\n",
      "  Batch 66,000  of  168,006.\n",
      "  Batch 68,000  of  168,006.\n",
      "  Batch 70,000  of  168,006.\n",
      "  Batch 72,000  of  168,006.\n",
      "  Batch 74,000  of  168,006.\n",
      "  Batch 76,000  of  168,006.\n",
      "  Batch 78,000  of  168,006.\n",
      "  Batch 80,000  of  168,006.\n",
      "  Batch 82,000  of  168,006.\n",
      "  Batch 84,000  of  168,006.\n",
      "  Batch 86,000  of  168,006.\n",
      "  Batch 88,000  of  168,006.\n",
      "  Batch 90,000  of  168,006.\n",
      "  Batch 92,000  of  168,006.\n",
      "  Batch 94,000  of  168,006.\n",
      "  Batch 96,000  of  168,006.\n",
      "  Batch 98,000  of  168,006.\n",
      "  Batch 100,000  of  168,006.\n",
      "  Batch 102,000  of  168,006.\n",
      "  Batch 104,000  of  168,006.\n",
      "  Batch 106,000  of  168,006.\n",
      "  Batch 108,000  of  168,006.\n",
      "  Batch 110,000  of  168,006.\n",
      "  Batch 112,000  of  168,006.\n",
      "  Batch 114,000  of  168,006.\n",
      "  Batch 116,000  of  168,006.\n",
      "  Batch 118,000  of  168,006.\n",
      "  Batch 120,000  of  168,006.\n",
      "  Batch 122,000  of  168,006.\n",
      "  Batch 124,000  of  168,006.\n",
      "  Batch 126,000  of  168,006.\n",
      "  Batch 128,000  of  168,006.\n",
      "  Batch 130,000  of  168,006.\n",
      "  Batch 132,000  of  168,006.\n",
      "  Batch 134,000  of  168,006.\n",
      "  Batch 136,000  of  168,006.\n",
      "  Batch 138,000  of  168,006.\n",
      "  Batch 140,000  of  168,006.\n",
      "  Batch 142,000  of  168,006.\n",
      "  Batch 144,000  of  168,006.\n",
      "  Batch 146,000  of  168,006.\n",
      "  Batch 148,000  of  168,006.\n",
      "  Batch 150,000  of  168,006.\n",
      "  Batch 152,000  of  168,006.\n",
      "  Batch 154,000  of  168,006.\n",
      "  Batch 156,000  of  168,006.\n",
      "  Batch 158,000  of  168,006.\n",
      "  Batch 160,000  of  168,006.\n",
      "  Batch 162,000  of  168,006.\n",
      "  Batch 164,000  of  168,006.\n",
      "  Batch 166,000  of  168,006.\n",
      "  Batch 168,000  of  168,006.\n",
      "  Average training loss: 0.13\n",
      "Running Validation...\n",
      "  Accuracy: 0.96\n",
      "  Validation Loss: 0.11\n",
      "======== Epoch 2 / 4 ========\n",
      "  Batch 2,000  of  168,006.\n",
      "  Batch 4,000  of  168,006.\n",
      "  Batch 6,000  of  168,006.\n",
      "  Batch 8,000  of  168,006.\n",
      "  Batch 10,000  of  168,006.\n",
      "  Batch 12,000  of  168,006.\n",
      "  Batch 14,000  of  168,006.\n",
      "  Batch 16,000  of  168,006.\n",
      "  Batch 18,000  of  168,006.\n",
      "  Batch 20,000  of  168,006.\n",
      "  Batch 22,000  of  168,006.\n",
      "  Batch 24,000  of  168,006.\n",
      "  Batch 26,000  of  168,006.\n",
      "  Batch 28,000  of  168,006.\n",
      "  Batch 30,000  of  168,006.\n",
      "  Batch 32,000  of  168,006.\n",
      "  Batch 34,000  of  168,006.\n",
      "  Batch 36,000  of  168,006.\n",
      "  Batch 38,000  of  168,006.\n",
      "  Batch 40,000  of  168,006.\n",
      "  Batch 42,000  of  168,006.\n",
      "  Batch 44,000  of  168,006.\n",
      "  Batch 46,000  of  168,006.\n",
      "  Batch 48,000  of  168,006.\n",
      "  Batch 50,000  of  168,006.\n",
      "  Batch 52,000  of  168,006.\n",
      "  Batch 54,000  of  168,006.\n",
      "  Batch 56,000  of  168,006.\n",
      "  Batch 58,000  of  168,006.\n",
      "  Batch 60,000  of  168,006.\n",
      "  Batch 62,000  of  168,006.\n",
      "  Batch 64,000  of  168,006.\n",
      "  Batch 66,000  of  168,006.\n",
      "  Batch 68,000  of  168,006.\n",
      "  Batch 70,000  of  168,006.\n",
      "  Batch 72,000  of  168,006.\n",
      "  Batch 74,000  of  168,006.\n",
      "  Batch 76,000  of  168,006.\n",
      "  Batch 78,000  of  168,006.\n",
      "  Batch 80,000  of  168,006.\n",
      "  Batch 82,000  of  168,006.\n",
      "  Batch 84,000  of  168,006.\n",
      "  Batch 86,000  of  168,006.\n",
      "  Batch 88,000  of  168,006.\n",
      "  Batch 90,000  of  168,006.\n",
      "  Batch 92,000  of  168,006.\n",
      "  Batch 94,000  of  168,006.\n",
      "  Batch 96,000  of  168,006.\n",
      "  Batch 98,000  of  168,006.\n",
      "  Batch 100,000  of  168,006.\n",
      "  Batch 102,000  of  168,006.\n",
      "  Batch 104,000  of  168,006.\n",
      "  Batch 106,000  of  168,006.\n",
      "  Batch 108,000  of  168,006.\n",
      "  Batch 110,000  of  168,006.\n",
      "  Batch 112,000  of  168,006.\n",
      "  Batch 114,000  of  168,006.\n",
      "  Batch 116,000  of  168,006.\n",
      "  Batch 118,000  of  168,006.\n",
      "  Batch 120,000  of  168,006.\n",
      "  Batch 122,000  of  168,006.\n",
      "  Batch 124,000  of  168,006.\n",
      "  Batch 126,000  of  168,006.\n",
      "  Batch 128,000  of  168,006.\n",
      "  Batch 130,000  of  168,006.\n",
      "  Batch 132,000  of  168,006.\n",
      "  Batch 134,000  of  168,006.\n",
      "  Batch 136,000  of  168,006.\n",
      "  Batch 138,000  of  168,006.\n",
      "  Batch 140,000  of  168,006.\n",
      "  Batch 142,000  of  168,006.\n",
      "  Batch 144,000  of  168,006.\n",
      "  Batch 146,000  of  168,006.\n",
      "  Batch 148,000  of  168,006.\n",
      "  Batch 150,000  of  168,006.\n",
      "  Batch 152,000  of  168,006.\n",
      "  Batch 154,000  of  168,006.\n",
      "  Batch 156,000  of  168,006.\n",
      "  Batch 158,000  of  168,006.\n",
      "  Batch 160,000  of  168,006.\n",
      "  Batch 162,000  of  168,006.\n",
      "  Batch 164,000  of  168,006.\n",
      "  Batch 166,000  of  168,006.\n",
      "  Batch 168,000  of  168,006.\n",
      "  Average training loss: 0.09\n",
      "Running Validation...\n",
      "  Accuracy: 0.96\n",
      "  Validation Loss: 0.10\n",
      "======== Epoch 3 / 4 ========\n",
      "  Batch 2,000  of  168,006.\n",
      "  Batch 4,000  of  168,006.\n",
      "  Batch 6,000  of  168,006.\n",
      "  Batch 8,000  of  168,006.\n",
      "  Batch 10,000  of  168,006.\n",
      "  Batch 12,000  of  168,006.\n",
      "  Batch 14,000  of  168,006.\n",
      "  Batch 16,000  of  168,006.\n",
      "  Batch 18,000  of  168,006.\n",
      "  Batch 20,000  of  168,006.\n",
      "  Batch 22,000  of  168,006.\n",
      "  Batch 24,000  of  168,006.\n",
      "  Batch 26,000  of  168,006.\n",
      "  Batch 28,000  of  168,006.\n",
      "  Batch 30,000  of  168,006.\n",
      "  Batch 32,000  of  168,006.\n",
      "  Batch 34,000  of  168,006.\n",
      "  Batch 36,000  of  168,006.\n",
      "  Batch 38,000  of  168,006.\n",
      "  Batch 40,000  of  168,006.\n",
      "  Batch 42,000  of  168,006.\n",
      "  Batch 44,000  of  168,006.\n",
      "  Batch 46,000  of  168,006.\n",
      "  Batch 48,000  of  168,006.\n",
      "  Batch 50,000  of  168,006.\n",
      "  Batch 52,000  of  168,006.\n",
      "  Batch 54,000  of  168,006.\n",
      "  Batch 56,000  of  168,006.\n",
      "  Batch 58,000  of  168,006.\n",
      "  Batch 60,000  of  168,006.\n",
      "  Batch 62,000  of  168,006.\n",
      "  Batch 64,000  of  168,006.\n",
      "  Batch 66,000  of  168,006.\n",
      "  Batch 68,000  of  168,006.\n",
      "  Batch 70,000  of  168,006.\n",
      "  Batch 72,000  of  168,006.\n",
      "  Batch 74,000  of  168,006.\n",
      "  Batch 76,000  of  168,006.\n",
      "  Batch 78,000  of  168,006.\n",
      "  Batch 80,000  of  168,006.\n",
      "  Batch 82,000  of  168,006.\n",
      "  Batch 84,000  of  168,006.\n",
      "  Batch 86,000  of  168,006.\n",
      "  Batch 88,000  of  168,006.\n",
      "  Batch 90,000  of  168,006.\n",
      "  Batch 92,000  of  168,006.\n",
      "  Batch 94,000  of  168,006.\n",
      "  Batch 96,000  of  168,006.\n",
      "  Batch 98,000  of  168,006.\n",
      "  Batch 100,000  of  168,006.\n",
      "  Batch 102,000  of  168,006.\n",
      "  Batch 104,000  of  168,006.\n",
      "  Batch 106,000  of  168,006.\n",
      "  Batch 108,000  of  168,006.\n",
      "  Batch 110,000  of  168,006.\n",
      "  Batch 112,000  of  168,006.\n",
      "  Batch 114,000  of  168,006.\n",
      "  Batch 116,000  of  168,006.\n",
      "  Batch 118,000  of  168,006.\n",
      "  Batch 120,000  of  168,006.\n",
      "  Batch 122,000  of  168,006.\n",
      "  Batch 124,000  of  168,006.\n",
      "  Batch 126,000  of  168,006.\n",
      "  Batch 128,000  of  168,006.\n",
      "  Batch 130,000  of  168,006.\n",
      "  Batch 132,000  of  168,006.\n",
      "  Batch 134,000  of  168,006.\n",
      "  Batch 136,000  of  168,006.\n",
      "  Batch 138,000  of  168,006.\n",
      "  Batch 140,000  of  168,006.\n",
      "  Batch 142,000  of  168,006.\n",
      "  Batch 144,000  of  168,006.\n",
      "  Batch 146,000  of  168,006.\n",
      "  Batch 148,000  of  168,006.\n",
      "  Batch 150,000  of  168,006.\n",
      "  Batch 152,000  of  168,006.\n",
      "  Batch 154,000  of  168,006.\n",
      "  Batch 156,000  of  168,006.\n",
      "  Batch 158,000  of  168,006.\n",
      "  Batch 160,000  of  168,006.\n",
      "  Batch 162,000  of  168,006.\n",
      "  Batch 164,000  of  168,006.\n",
      "  Batch 166,000  of  168,006.\n",
      "  Batch 168,000  of  168,006.\n",
      "  Average training loss: 0.08\n",
      "Running Validation...\n",
      "  Accuracy: 0.97\n",
      "  Validation Loss: 0.10\n",
      "======== Epoch 4 / 4 ========\n",
      "  Batch 2,000  of  168,006.\n",
      "  Batch 4,000  of  168,006.\n",
      "  Batch 6,000  of  168,006.\n",
      "  Batch 8,000  of  168,006.\n",
      "  Batch 10,000  of  168,006.\n",
      "  Batch 12,000  of  168,006.\n",
      "  Batch 14,000  of  168,006.\n",
      "  Batch 16,000  of  168,006.\n",
      "  Batch 18,000  of  168,006.\n",
      "  Batch 20,000  of  168,006.\n",
      "  Batch 22,000  of  168,006.\n",
      "  Batch 24,000  of  168,006.\n",
      "  Batch 26,000  of  168,006.\n",
      "  Batch 28,000  of  168,006.\n",
      "  Batch 30,000  of  168,006.\n",
      "  Batch 32,000  of  168,006.\n",
      "  Batch 34,000  of  168,006.\n",
      "  Batch 36,000  of  168,006.\n",
      "  Batch 38,000  of  168,006.\n",
      "  Batch 40,000  of  168,006.\n",
      "  Batch 42,000  of  168,006.\n",
      "  Batch 44,000  of  168,006.\n",
      "  Batch 46,000  of  168,006.\n",
      "  Batch 48,000  of  168,006.\n",
      "  Batch 50,000  of  168,006.\n",
      "  Batch 52,000  of  168,006.\n",
      "  Batch 54,000  of  168,006.\n",
      "  Batch 56,000  of  168,006.\n",
      "  Batch 58,000  of  168,006.\n",
      "  Batch 60,000  of  168,006.\n",
      "  Batch 62,000  of  168,006.\n",
      "  Batch 64,000  of  168,006.\n",
      "  Batch 66,000  of  168,006.\n",
      "  Batch 68,000  of  168,006.\n",
      "  Batch 70,000  of  168,006.\n",
      "  Batch 72,000  of  168,006.\n",
      "  Batch 74,000  of  168,006.\n",
      "  Batch 76,000  of  168,006.\n",
      "  Batch 78,000  of  168,006.\n",
      "  Batch 80,000  of  168,006.\n",
      "  Batch 82,000  of  168,006.\n",
      "  Batch 84,000  of  168,006.\n",
      "  Batch 86,000  of  168,006.\n",
      "  Batch 88,000  of  168,006.\n",
      "  Batch 90,000  of  168,006.\n",
      "  Batch 92,000  of  168,006.\n",
      "  Batch 94,000  of  168,006.\n",
      "  Batch 96,000  of  168,006.\n",
      "  Batch 98,000  of  168,006.\n",
      "  Batch 100,000  of  168,006.\n",
      "  Batch 102,000  of  168,006.\n",
      "  Batch 104,000  of  168,006.\n",
      "  Batch 106,000  of  168,006.\n",
      "  Batch 108,000  of  168,006.\n",
      "  Batch 110,000  of  168,006.\n",
      "  Batch 112,000  of  168,006.\n",
      "  Batch 114,000  of  168,006.\n",
      "  Batch 116,000  of  168,006.\n",
      "  Batch 118,000  of  168,006.\n",
      "  Batch 120,000  of  168,006.\n",
      "  Batch 122,000  of  168,006.\n",
      "  Batch 124,000  of  168,006.\n",
      "  Batch 126,000  of  168,006.\n",
      "  Batch 128,000  of  168,006.\n",
      "  Batch 130,000  of  168,006.\n",
      "  Batch 132,000  of  168,006.\n",
      "  Batch 134,000  of  168,006.\n",
      "  Batch 136,000  of  168,006.\n",
      "  Batch 138,000  of  168,006.\n",
      "  Batch 140,000  of  168,006.\n",
      "  Batch 142,000  of  168,006.\n",
      "  Batch 144,000  of  168,006.\n",
      "  Batch 146,000  of  168,006.\n",
      "  Batch 148,000  of  168,006.\n",
      "  Batch 150,000  of  168,006.\n",
      "  Batch 152,000  of  168,006.\n",
      "  Batch 154,000  of  168,006.\n",
      "  Batch 156,000  of  168,006.\n",
      "  Batch 158,000  of  168,006.\n",
      "  Batch 160,000  of  168,006.\n",
      "  Batch 162,000  of  168,006.\n",
      "  Batch 164,000  of  168,006.\n",
      "  Batch 166,000  of  168,006.\n",
      "  Batch 168,000  of  168,006.\n",
      "  Average training loss: 0.07\n",
      "Running Validation...\n",
      "  Accuracy: 0.97\n",
      "  Validation Loss: 0.10\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gradient_accumulation_steps = 4\n",
    "\n",
    "# Store the average loss after each epoch so we can plot them.\n",
    "loss_values = []\n",
    "\n",
    "for epoch_i in range(0, epochs):\n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    \n",
    "    # Perform one full pass over the training set.\n",
    "    \n",
    "    print(\n",
    "        '======== Epoch {:} / {:} ========'.format(\n",
    "            epoch_i + 1, \n",
    "            epochs\n",
    "        )\n",
    "    )\n",
    "    # empty gpu cache to free memory\n",
    "    torch.cuda.empty_cache()\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_train_loss = 0\n",
    "\n",
    "    # Put the model into training mode. Don't be mislead--the call to \n",
    "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
    "    # `dropout` and `batchnorm` layers behave differently during training\n",
    "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
    "    model.train()\n",
    "\n",
    "    # For each batch of training data...\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # Progress update every 2000 batches.\n",
    "        if step % 2000 == 0 and not step == 0:\n",
    "            # Report progress.\n",
    "            print(\n",
    "                '  Batch {:>5,}  of  {:>5,}.'.format(\n",
    "                    step, \n",
    "                    len(train_dataloader)\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # Unpack this training batch from our dataloader. \n",
    "        #\n",
    "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
    "        # `to` method.\n",
    "        #\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)     \n",
    "\n",
    "        # Perform a forward pass (evaluate the model on this training batch).\n",
    "        # This will return the loss (rather than the model output) because we\n",
    "        # have provided the `labels`.\n",
    "        # The documentation for this `model` function is here: \n",
    "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "        outputs = model(\n",
    "            b_input_ids, \n",
    "            token_type_ids=None, \n",
    "            attention_mask=b_input_mask, \n",
    "            labels=b_labels\n",
    "        )\n",
    "\n",
    "        # Then access the loss and logits directly\n",
    "        loss = outputs.loss\n",
    "        loss /= gradient_accumulation_steps \n",
    "        #logits = outputs.logits\n",
    "\n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.backward()\n",
    "        \n",
    "        # Accumulate the training loss over all of the batches so that we can\n",
    "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
    "        # single value; the `.item()` function just returns the Python value \n",
    "        # from the tensor.\n",
    "        total_train_loss += loss.item() * gradient_accumulation_steps\n",
    "\n",
    "        # Perform optimization step every 'gradient_accumulation_steps' batches\n",
    "        if (step + 1) % gradient_accumulation_steps == 0:\n",
    "            # Clip the norm of the gradients to 1.0 to prevent the \"exploding gradients\" problem\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            # Update parameters\n",
    "            optimizer.step()\n",
    "            # Update the learning rate.\n",
    "            scheduler.step() # Uncomment if using a learning rate scheduler\n",
    "            # Clear the gradients\n",
    "            model.zero_grad()          \n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    # Store the loss value for plotting the learning curve.\n",
    "    loss_values.append(avg_train_loss)\n",
    "    \n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "\n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    # Put the model in evaluation mode--the dropout layers behave differently\n",
    "    # during evaluation.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables\n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in validation_dataloader:\n",
    "        \n",
    "        # Unpack this training batch from our dataloader. \n",
    "        #\n",
    "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
    "        # `to` method.\n",
    "        #\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        \n",
    "        # Telling the model not to compute or store gradients, saving memory and\n",
    "        # speeding up validation\n",
    "        with torch.no_grad():        \n",
    "            # Forward pass, calculate logit predictions.\n",
    "            # This will return the logits rather than the loss because we have\n",
    "            # not provided labels.\n",
    "            # token_type_ids is the same as the \"segment ids\", which \n",
    "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "            # The documentation for this `model` function is here: \n",
    "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "            outputs = model(\n",
    "                b_input_ids, \n",
    "                token_type_ids=None, \n",
    "                attention_mask=b_input_mask,\n",
    "                labels=b_labels\n",
    "            )\n",
    "        \n",
    "        # access the loss\n",
    "        loss = outputs.loss\n",
    "        # update total evaluation loss\n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = outputs.logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        # Calculate the accuracy for this batch of test sentences, and\n",
    "        # accumulate it over all batches.\n",
    "        preds = np.argmax(logits, axis=1).flatten()\n",
    "        total_eval_accuracy += np.sum(preds == label_ids)\n",
    "\n",
    "    # Report the final accuracy for this validation run.\n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader.dataset)\n",
    "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "334b5f16-2cd9-4184-a48f-5cffb5aa4753",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('model_save/test1\\\\tokenizer_config.json',\n",
       " 'model_save/test1\\\\special_tokens_map.json',\n",
       " 'model_save/test1\\\\vocab.txt',\n",
       " 'model_save/test1\\\\added_tokens.json')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the model to the path of your choice, for example 'model_save/'\n",
    "model.save_pretrained('model_save/test1')\n",
    "tokenizer.save_pretrained('model_save/test1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4709cb-b9f6-4c40-98db-7c73d0f22ab0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
